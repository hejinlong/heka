第3章：配置
=====
一个Heka全新安装是一张白纸，充满各种想象，就看你如何描绘。其中的挑战是新手如何很轻松的利用非常灵活的工具，理解各种特性和选项。本文尝试通过运行一个hekad守护进程的配置和一些Heka普通应用案例，希望提供尽可能多的文档在实际开发过程中参考。
当我们完成配置的时候Heka执行下面的任务：
* statsd客户端通过UDP接收数据。.
* 转发聚合统计数据在原数据服务器和InfluxDB服务器之间。
* 生成一组特定statsd统计的实时图表。
* 加载和解析nginx访问日志文件。
* 加载Nginx日志文件生成JSON数据结构且发送到ElasticSearch数据库集群。
* 生成Nginx访问日志HTTP请求状态码实时统计图表。
* 执行HTTP状态码数据基本算法异常检测，有问题通过电子邮件发送消息通知。
在深入研究之前，让我们通过一个非常简单的配置来确保一切正常工作。

3.1最简单的Heka配置
------
一个最简单的Heka配置是从本地文件系统加载单个文件，并且输出该文件内容。下面是配置示例。
```
[LogstreamerInput]
log_directory="/var/log"
file_match='auth\.log'

[PayloadEncoder]
append_newlines=false

[LogOutput]
message_matcher="TRUE"
encoder="PayloadEncoder"
```

Heka是通过一个或多个TOML格式配置文件来配置，每个配置文件由一个或多个部分构成。上述结构包括三个部分，第一个指定LogstreamerInput，Heka主要机制是从本地文件系统加载文件。该配置是加载/var/log/auth.log，但是你可以修改它，通过修改log_directory配置文件所在目录来加载任何文件，另外file_match配置一个正则表达式与唯一的文件名匹配。注意单引号(‘auth\.log’)和双引号(“auth\\.log”)。
在大多数现实情况下，一个LogstreamerInput将包括一个解码器的设置，这将解析该文件的内容，以从文本格式提取数据，并将它们映射到Heka消息模式。然而在这种情况下，我们仍然使用的默认行为，Heka为每一行日志文件创建新的消息，存储到文本日志行作为Heka消息的有效负荷。
接下来两部分告诉Heka用LogstreamerInput生成的消息来做什么。LogOutput是简单的写数据到Hekastdout过程。我们设置message_matcher=“TRUE”来指定这个输出应该捕获经过Heka管道的每一条消息。编码器设置告诉Heka使用我们已经配置的PayloadEncoder，它从每条捕获消息中提取有效负载，并将其用作原始数据输出。
查看是否有Heka系统功能，你可以新建一个sanity_check.toml文件并且粘贴以上配置，如果有必要调整LogstreamerInput的设置，以指向另一个文件。然后你可以运行Heka应用hekad-config=/path/to/sanity_check.toml，并且你应该看到打印到控制台的日志文件内容。如果你正在加载任何写入日志文件的新消息行，Heka会通知并写入你正在加载的日志文件，实时输出到stdout。
注意LogstreamerInput跟踪多大程度上在3个文档取得，所以如果你使用Ctrl+C停止Heka，再重新启动就不会看到相同的数据。Heka在“seekjournal”文件中保存当前位置，默认在/var/cache/hekad/logstreamer/LogstreamerInput。如果删除此文件，然后重新启动Heka你应该看到它再次开始加载整个文件。
恭喜！现在你成功的基于一个完整的、正在工作的配置运行Heka。但显然也有更简单的工具可以使用，如果你想要做的是写入日志文件的内容到标准输出。现在，我们已经得到了初步成功，让我们更加深入地了解更加复杂的Heka配置来处理许多实际工作中的用例。

3.2全局配置
------
如上所述，Heka是用TOML配置文件来进行配置。TOML配置的大部分包括Heka某个插件的相关信息，但是一部分标注hekad允许配置一些Heka的全局配置选项。在许多情况下大多数这些选项的默认值就够了，你的配置根本不需要hekad部分。然而有些选项值需要注意：
* maxprocs(int,默认1):
该设置对应Go的GOMAXPROCS环境变量。它指定hekad进程将被允许使用多少个CPU内核。该设置最佳选项依赖许多因素诸如Heka进程数量，Heka运行机器上的内核数量，以及该机器执行的其他任务。对于专用的Heka聚合机器，通常可以等于可用的CPU内核数量，或内核数量减去1，而Heka进程运行在一两个CPU内核可能是一个比较好的选择。
* base_dir(string,默认‘/var/cache/hekad’或‘c:\var\cache\hekad’):
除了配置文件的位置外，还有两个目录对于hekad进程运行很重要。第一个叫base_dir，它是Heka存储对其功能至关重要的信息工作目录，例如seekjournal文件以跟踪日志流中的当前位置，或者意味着在Heka重启之间存在的沙箱过滤器聚集数据。当然重要的是正在运行hekad进程的用户对base_dir有访问权。
* share_dir(string,默认‘/usr/share/heka’或‘c:\usr\share\heka’):
Heka功能的第二个重要的目录叫share_dir。这是Heka希望找到它需要的静态资源的地方，例如用于仪表盘输出的HTML/javascript源码，或各种基于Lua插件的源代码。用户拥有hekad进程对该目录有访问权限，但不应该有可写权限。
值得注意的是，尽管Heka原本期望在base_dir和/或share_dir文件夹中找到某些资源，但在插件配置中几乎总是可以根据具体情况覆盖特定资源的位置。例如，SandboxFilter中的文件名选项指定该过滤器的Lua源代码的文件系统路径。如果指定为相对路径，则将相对于share_dir计算路径。如果指定为绝对路径，则绝对路径将被保留。
对于我们的示例，我们将保留大多数全局选项的默认值，但是我们会将maxprocs设置从1改为2，以便我们可以获得并行行为：
```
[hekad]
maxprocs=2
```

3.3接收Statsd数据
------
一旦我们配置了Heka的全局设置，就可以开始插件设置。我们要解决的第一件事是让Heka设置为接受来自statsd客户端的数据。这涉及两个不同的插件，一个接受网络连接并解析接收到统计数据的StatsdInput，以及一个StatAccumulatorInput，它将接受StatsdInput收集的数据，执行必要的聚合，并定期生成的‘statmetric’消息包含聚合数据。
这些插件的配置相当简单：
```
[StatsdInput]

[StatAccumInput]
ticker_interval=1
emit_in_fields=true
```
这个两部分TOML告诉Heka应该包括一个StatsdInput和一个StatAccumInput。StatsdInput每个配置设置使用默认值，而StatAccumInput覆盖它设置的两个默认值。ticker_interval=1设置意味着statmetric消息从原来的每秒生成一次更改为每5秒生成一次，而emit_in_fields=true设置意味着聚合统计数据嵌入在生成statmetric消息的动态字段中，除了在payload消息中嵌入到graphite文本格式的默认值。
这可能看起来很简单，但实际上有一些细节隐藏在里面，这很重要。首先，它不是立即显而易见，但有两个插件之间的显式连接。StatsdInput有一个stat_accum_name设置，我们不需要设置，因为它默认为“StatAccumInput”。以下配置完全等效：
```
[StatsdInput]
stat_accum_name="StatAccumInput"

[StatAccumInput]
ticker_interval=1
emit_in_fields=true
```
下一个要注意的是，我们使用了一个公共的Heka配置简写，通过在TOML段标题中嵌入名称和类型。如果你不需要的话可以使用一个和类型分开的名字，Heka允许你这样做是为了方便。但这不是必须的，可以给插件一个不同的名称，在TOML节中表示类型，而不是在其标题中：
```
[statsd_input]
type="StatsdInput"
stat_accum_name="stat_accumulator"

[stat_accumulator]
type="StatAccumInput"
ticker_interval=1
emit_in_fields=true
```
上面的配置与原来的有两点不同，因为插件现在有不同的名称标识符，但功能上的行为与以前的版本是一样的。在需要多个插件类型实例的情况下，能够从其类型中分离插件名称是很重要的。例如，如果你想让第二个StatsdInput监听8126端口，并用8125端口上的默认值，使用以下配置：
```
[statsd_input_8125]
type="StatsdInput"
stat_accum_name="stat_accumulator"

[statsd_input_8126]
type="StatsdInput"
stat_accum_name="stat_accumulator"
address="127.0.0.1:8126"

[stat_accumulator]
type="StatAccumInput"
ticker_interval=1
emit_in_fields=true
```
我们不需要两个StatsdInputs作为例子，但是为了简单会用最简洁的配置。

3.4转发汇总统计数据
------
单独收集统计信息实际上不能提供太多价值，我们希望能够查看已收集的数据。Statsd服务器通常用于聚合入站统计信息，然后周期性地将合计数量传送到上游时间序列数据库，通常为Graphite，尽管InfluxDB正在迅速普及。对于Heka来说替换一个独立的statsd服务器，需要能够做同样的事情。
要了解这将如何工作，我们需要回顾一下，看看Heka如何处理消息路由。首先，数据通过输入插件进入Heka管道。然后，它需要从其原始格式转换为Heka知道如何使用的消息对象。通常这是使用解码器插件，虽然在上面的statsd示例，而StatAccumInput本身周期性地生成statmetric消息。
要了解如何工作，我们需要回顾一下，看看Heka如何处理消息路由。首先，数据通过输入插件进入Heka管道。然后，它需要从其原始格式转换为Heka能够识别如何使用的消息对象。通常这是使用解码器插件，虽然在上面的statsd示例，而StatAccumInput本身周期性地生成statmetric消息。
在数据被封送到一个（或更多）消息后，这些消息被Heka的内部消息路由处理。消息路由器然后将遍历所有注册的过滤器和输出插件，以查看哪些想要处理消息。每个过滤器和输出提供消息匹配器以指定其想接收的消息。路由器将每个消息传递给每个消息匹配器，如果有匹配，则它们的匹配器将消息传递给插件。
回到我们的示例，我们将开始设置一个CarbonOutput插件，知道如何传递消息到上游Graphite Carbon服务器。我们将其配置为接收StatAccumInput生成的statmetric消息：
```
[CarbonOutput]
message_matcher="Type=='heka.statmetric'"
address="mycarbonserver.example.com:2003"
protocol="udp"
```
通过Type字段路由的消息等于heka.statmetric（这是默认情况下StatAccumOutput发出的）将传递到此输出，然后它将通过UDP将其传递到指定的carbon服务器地址。这很简单，但它是一个基本概念。Heka中几乎所有通信都使用通过消息路由器传递的Heka消息对象，并且与注册的匹配器匹配。
好，让我们和Graphite谈谈。什么是InfluxDB？InfluxDB有一个扩展，允许它支持graphite格式，所以我们可以使用，只是设置了第二个CarbonOutput：
```
[carbon]
type="CarbonOutput"
message_matcher="Type=='heka.statmetric'"
address="mycarbonserver.example.com:2003"
protocol="udp"

[influx]
type="CarbonOutput"
message_matcher="Type=='heka.statmetric'"
address="myinfluxserver.example.com:2003"
protocol="udp"
```
这里有几件事需要做。首先，不要因为指定插件是我们正在配置的类型的type=“CarbonOutput”而感到困惑，在message_matcher=“Type=='heka.statmetric'”中的“Type”是指通过Heka路由的消息Type字段。它们都称为“Type”，但除了名字之外其他都是无关的。
其次，你会看到，有多个输出（和/或过滤器）插件具有相同的message_matcher设置。路由器不在乎，它会愉快地给予它们相同的消息和任何其他合适的匹配。
正常工作时它只是很好使用InfluxDB本地HTTP  API。为此，我们可以使用方便的HttpOutput：
```
[CarbonOutput]
message_matcher="Type=='heka.statmetric'"
address="mycarbonserver.example.com:2003"
protocol="udp"

[statmetric_influx_encoder]
type="SandboxEncoder"
filename="lua_encoders/statmetric_influx.lua"

[influx]
type="HttpOutput"
message_matcher="Type=='heka.statmetric'"
address="http://myinfluxserver.example.com:8086/db/stats/series"
encoder="statmetric_influx_encoder"
username="influx_username"
password="influx_password"
```
上面的HttpOutput配置还将捕获statmetric消息，然后通过HTTP将数据传递到InfluxDB正在监听的指定地址。可是等等！什么是statmetric-influx-encoder的东西？我很高兴你这样问……

3.5编译器插件
------
我们已经简要介绍了原始数据如何转换为Heka的路由器，过滤器和输出能够处理的标准消息格式。类似地在输出方式上，必须从标准消息格式中提取数据，并将其序列化为目的地所需的任何格式。这通常通过使用编码器插件来实现，编码器插件将Heka消息作为输入，并生成为输出插件可通过线路发送的输出原始字节。CarbonOutput不指定编码器，因为它假设Graphite数据将在消息有效负载中，StatAccumInput将其置于其中，但大多数输出需要指定一个编码器，因为它们知道接收的数据流如何生成消息。
在以上的InfluxDB示例中，你可以看到我们已经定义的statmetric_influx_encoder,SandboxEncoder的一个类型。“Sandbox”插件的核心逻辑在Lua中实现并在沙箱中运行。Heka支持SandboxDecoder，SandboxFilter，和SandboxEncoder插件。在这种情况下，我们使用Heka提供的SandboxEncoder实现，它知道如何从heka.statmetric消息字段中提取数据，并使用该数据以InfluxDB识别的格式生成JSON（详见StatMetricInfluxDBEncoder）。.
编码器和输出插件之间关注的分歧是允许大量的灵活性。很容易开发自己的SandboxEncoder插件来生成任何需要的格式，允许相同的HttpOutput实现可以用于多个基于HTTP的后端请求，而不需要为每个服务单独的输出插件。此外，相同的编码器可以用于不同的输出。例如，如果我们想将InfluxDB格式的数据写入系统文件以供以后处理，我们可以使用statmetric_influx编码器与FileOutput来完成。

3.6实时统计图表
------
虽然Graphite和InfluxDB提供了显示它们接收统计数据图形的机制，但Heka也能够直接提供这些数据的图形。这些图形将实时更新，因为数据流经Heka，没有数据存储驱动图形的延迟。以下配置片段显示如何完成此操作：
```
[stat_graph]
type="SandboxFilter"
filename="lua_filters/stat_graph.lua"
ticker_interval=1
preserve_data=true
message_matcher="Type=='heka.statmetric'"

[stat_graph.config]
num_rows=300
secs_per_row=1
stats="stats.counters.000000.countstats.counters.000001.countstats.counters.000002.count"
stat_labels="counter_0counter_1counter_2"
preservation_version=0

[DashboardOutput]
ticker_interval=1
```
这里有一个很简单的配置，让我们一起了解发生了什么。首先，我们有一个stat_graph配置项，它告诉Heka启动一个SandboxFilter插件，一个过滤器插件用Lua实现处理代码。filename选项指向Heka附带的过滤器实现。该过滤器知道如何从statmetric消息中提取数据，并将该数据存储在循环缓冲区数据结构中。preserve_data选项告诉Heka，如果Heka关闭，这个过滤器中的所有全局数据（在这种情况下，循环缓冲区数据）应该刷新到磁盘，因此当Heka重新启动时，可以重新加载它。ticker_interval选项指定我们的过滤器将每秒发送一个包含cbuf数据的输出消息到路由器。此消息可以由其他过滤器和/或输出使用，例如我们的DashboardOutput，它将生成图形（如下）。
之后，我们有一个stat_graph.config部分。这不是指定一个新的插件，这是嵌套配置，外部stat_graph部分的一个子部分。（注意：段嵌套是通过在段名称中使用stat_graph。前缀来指定的，缩进有助于可读性，但对配置的语义没有影响。）stat-graph部分配置SandboxFilter并告诉它Lua源代码使用，stat_graph.config部分被传递到Lua源代码中，用于进一步定制过滤器的行为。
那么这个嵌套配置中包含什么？前两个选项num_rows和secs_per_row正在配置循环缓冲区数据结构，过滤器将使用它们来存储统计数据。将循环缓冲区数据结构视为电子表格可能会有所帮助。我们的电子表格将有300行，每行将表示一秒钟累积的数据，所以在任何给定的时间，我们将在过滤器中保存五分钟的统计数据。接下来的两个选项stats和stat_labels告诉Heka我们想要绘制哪些统计数据，并在图形中使用较短的标签。最后，preservation_version部分允许我们对数据结构进行版本化。这是因为我们的数据结构可能会改变。如果让这个过滤器运行一段时间，收集数据，然后关闭Heka，300行循环缓冲区数据会写入磁盘。如果你随后更改num_rows设置并尝试重新启动Heka，则过滤器将无法启动，因为保留数据的300行大小将不匹配我们指定的新尺寸。在这种情况下，将preservation_version值从0增加到1，告诉Heka保留的数据不再有效，并且应重新创建数据结构。

3.7 Heka仪表盘
------
有用的是注意到这一点，虽然SandboxFilter收集我们感兴趣的数据，并打包一个有用的图形绘制格式，它实际上不做任何图形。相反，它周期性地创建一个heka.sandbox-output类型的消息，包含当前循环缓冲区数据，并将该消息注入Heka的消息路由器。这是我们配置的仪表盘输出。
Heka的DashboardOutput默认配置为监听heka.sandbox输出消息（以及一些其他消息类型，我们现在将其忽略）。当它接收到沙箱输出消息时，将检查消息的内容，并且如果消息包含循环缓冲区数据，它将自动生成该数据的实时图。
默认情况下，通过将Web浏览器指向Heka正在运行的计算机的4352端口，可以使用仪表盘UI。你将看到的第一个页面是运行状况报告，其中提供了插件配置的概述，以及有关消息如何流过Heka管道的一些内容：
 
 
...并向下滚动页面...
 
在页面头部是一个Sandboxes链接，它将带你到所有运行的SandboxFilter插件的列表，以及它们发出的输出列表。点击它，我们可以看到ourstat_graph过滤器和Stats循环缓冲区（“CBUF”）输出：
 
如果点击过滤器名称stat_graph，你将看到一个页面，其中显示有关该插件性能的详细信息，包括已处理的邮件数量，邮件匹配器匹配邮件的平均时间，花在处理消息上的平均时间数量，等等：
 
最后，点击Stats链接，我们将看到实际渲染的输出，一个实时更新的曲线图，显示我们在stat_graphSandboxFilter配置中指定的特定计数器统计值：
 
通过调整我们现有stat_graph过滤器配置的stats和stat_labels值，可以将其他统计信息添加到此图形中，但是如果我们这样做，我们必须碰到preserve_version，告诉Heka先前的数据结构不再有效。你可以通过使用相同的stat_graph.lua源代码包括其他SandboxFilter节来创建多个图。
还应该提到的是，虽然我们使用的stat_graph.lua过滤器只使用一个输出图表，但是单个过滤器确实可以生成多个图。SandboxFilters还可以发出其他类型的输出，例如原始JSON数据，DashboardOutput非常顺利用作原始文本。这对于根据Heka正在处理的数据生成ad-hoc API端点非常有用。访问我们的Sandbox文档，了解有关使用我们的SandboxAPI编写自己的Lua过滤器更多信息。

3.8加载和解析Nginx日志文件
------
对于下一个技巧，我们将加载NginxHTTP服务器的访问日志文件，并提取关于其中记录的每个HTTP请求信息，以更结构化的方式将其存储在Heka消息的字段中。第一步是告诉Heka在哪里可以找到Nginx访问日志文件。除了Nginx日志通常不只是一个单一的文件，它是一系列文件受特定站点的轮换方案。例如，在系统中/var/log/nginx目录如下：
```
access.log
access.log.1
access.log.2.gz
access.log.3.gz
access.log.4.gz
access.log.5.gz
access.log.6.gz
access.log.7.gz
access.log.8.gz
access.log.9.gz
error.log
```
这是一个常见的轮换方案，但有很多其他的。在托管许多域的情况下，可能有几组日志文件，每个域一个，每个域通过文件和/或文件夹的名称区分。幸运的是Heka的LogstreamerInput提供了一个处理所有这些情况的机制。LogstreamerInput已经有了大量的文档，所以我们不会在这里详尽的细节介绍，而是显示一个示例配置，正确处理上述情况：
```
[nginx_access_logs]
type="LogstreamerInput"
splitter="TokenSplitter"
decoder="nginx_access_decoder"
log_directory="/var/log/nginx"
file_match='access\.log\.?(?P<Index>\d+)?(.gz)?'
priority=["^Index"]
```
上面的splitter选项告诉Heka每个记录将由一个字符标记分隔，在这种情况下是默认标记\n。如果我们的记录由不同的字符分隔，我们可以添加一个指定替换的令牌分割器部分。如果单个字符不足以找到我们的记录边界，例如在记录跨多行的情况下，我们可以使用正则表达式分割器来提供描述记录边界的正则表达式。log_directory选项指示我们感兴趣的文件在哪里。file_match是与构成日志流所有文件匹配的正则表达式。在这种情况下，他们都必须以access.log开头，然后他们可以（可选）后跟一个点（.），然后（可选，再次）一个或两个数字，然后（可选，多次）gzip扩展名（.gz）。找到的任何数字都将作为索引匹配组捕获，而priority选项指定我们使用此索引值来确定文件的顺序。字符（^）反转优先级的顺序，因为在我们的例子中，低位数字表示较新的文件。
LogstreamerInput将使用此配置数据来查找所有相关文件，然后它将开始工作，通过整个文件流从旧到新，跟踪其进度。如果Heka停止并重新启动，它将在停止时的地方拾取，即使该文件在Heka停止期间轮转。当它到达最新文件的末尾时，它将紧接着加载新行，因为它们被添加，并注意到文件被转化，所以它可以向前跳，开始加载新的一行。
然后将我们带到解码器选项。这告诉HekaLogstreamerInput将使用哪个解码器插件来解析加载的日志文件。nginx访问解码器配置如下：
```
[nginx_access_decoder]
type="SandboxDecoder"
filename="lua_decoders/nginx_access.lua"

[nginx_access_decoder.config]
log_format='$remote_addr-$remote_user[$time_local]"$request"$status$body_bytes_sent"$http_referer""$http_user_agent"'
type="nginx.access"
```
其中一些现在应该看起来很熟悉。这是一个SandboxDecoder，意味着它是一个解码器插件与在Lua中实现的实际解析逻辑。外部配置部分用来配置SandboxDecoder本身，而嵌套部分提供了传递到Lua代码的附加配置信息。
虽然可以自己编写自定义Lua解析代码，在这种情况下，我们再次使用由Heka提供的插件，专门用于解析Nginx访问日志。但是Nginx没有单个访问日志格式，确切的输出是由Nginx配置中的log_format指令动态指定的。幸运的是Heka的解码器相当复杂；解析访问日志输出是复制Nginx配置文件中相应的log_format指令，并将其粘贴到Heka解码器配置中的log_format选项。如上所述，Heka将使用LPEG的魔力动态创建一种语法，它将从日志行中提取数据并将它们存储在Heka消息字段中。最后，上面的类型选择允许你指定在此解码器生成的消息上应该设置Type字段。

3.9发送Nginx数据到ElasticSearch
------
人们感兴趣的一个常见用例是将从其HTTP服务器日志中提取的数据发送到ElasticSearch，以便他们可以使用由优秀的仪表盘创建工具Kibana。生成的仪表盘来仔细阅读该数据。我们已经使用上面的输入和解码器配置处理了加载和解析信息，现在让我们看看另一面有以下输出和编码器设置：
```
[ESJsonEncoder]
es_index_from_timestamp=true
type_name="%{Type}"

[ElasticSearchOutput]
server="elasticsearch.example.com:9200"
message_matcher="Type=='nginx.access'"
encoder="ESJsonEncoder"
flush_interval=50
```
逆向工作，我们先看看ElasticSearchOutput配置。服务器设置ElasticSearch正在监听的位置。message_matcher告诉我们，我们将捕获类型值为nginx.access的消息，你可以记住它是在我们上面讨论的解码器配置中设置。flush_interval设置指定我们将在输出中批处理我们的记录，并且每50毫秒将它们刷新到ElasticSearch。

这留给我们的编码器设置，以及相应的ElasticSearchJSONEncoder部分。ElasticSearchOutput使用ElasticSearch的BulkAPI告诉ElasticSearch如何对文档编制索引，这意味着每个文档插入都包含一个满足BulkAPI的小型JSON对象，后面是包含文档本身的另一个JSON对象。Heka提供的编码器，它们将从Heka消息中提取数据并生成一个适当的BulkAPI头，我们上面使用的ElasticSearchJSONEncoder，它基于消息的模式生成一个干净的文档模式编码；ElasticSearchLogstashV0Encoder，它使用由Logstash定义的“v0”模式格式（专门用于HTTP请求数据，由Kibana本地支持）和ElasticSearchPayloadEncoder，它假定消息有效内容已经包含完全形成的JSON文档准备发送到ElasticSearch，并且只需添加必要的BulkAPI分段。
在我们的ESJsonEncoder部分中，我们主要遵循默认设置。默认情况下，该解码器根据当前日期将文档插入到ElasticSearch索引中：heka-YYYY.MM.DD（在配置中拼写为heka-％{2006.01.02}）。Thees_index_from_timestamp=true选项指示Heka在确定要用于索引名称的日期时使用消息中的时间戳，而不是使用系统时钟的当前时间作为基础的默认行为。type选项告诉Heka每个记录应该使用ElasticSearch记录类型。该选项支持从消息对象写入各种值；在上面的示例中，消息的类型字段将用作ElasticSearch记录类型名称。

3.10生成HTTP状态码图
------
ElasticSearch和Kibana提供了一些很好的工具，用于绘制和查询从Nginx日志中解析的HTTP请求数据，但是，与上面的统计数据一样，Heka很好的获得这些数据的实时图形。正如你可能猜到的，Heka已经为此提供了专门的插件：
```
[http_status]
type="SandboxFilter"
filename="lua_filters/http_status.lua"
ticker_interval=1
preserve_data=true
message_matcher="Type=='nginx.access'"

[http_status.config]
sec_per_row=1
rows=1800
perservation_version=0
```
如前所述，Heka中的图形化是通过发送包含循环缓冲区数据的过滤器来过滤消息，消耗这些消息并在图形上显示数据的DashboardOutput协作来实现的。我们之前已经配置了一个DashboardOutput，现在我们只需要添加一个捕获nginx.access消息过滤器，并将数据聚合到循环缓冲区中。
Heka有一个标准消息格式，它用于表示单个HTTP请求的数据，由解析我们的日志文件的Nginx访问日志解码器使用。在此格式中，HTTP响应的状态代码存储在称为状态的动态消息字段中。以上过滤器将创建一个循环缓冲区数据结构，以将这些响应状态代码存储为6列：100s，200s，300s，400s，500s和未知。与之前类似，嵌套配置告诉过滤器要在循环缓冲区中保留多少行数据以及每行应该表示多少MB数据。它也给我们一个preserve_version，所以我们可以标记数据结构发生变化。
一旦我们将此部分添加到配置并重新启动hekad，我们应该能够浏览到仪表盘UI，并且能够找到从我们的HTTP服务器日志中提取的各种响应状态类别的图形。

3.11异常检测
------
我们正在接近旅行的尽头。我们要收集的所有数据现在流经Heka，被传送到外部数据存储用于离线处理和分析，并通过Heka的仪表盘以实时图形显示。我们要激活的唯一剩余行为是异常检测，并且基于检测到的异常事件生成通知程序。我们将从异常检测部分开始。
我们已经讨论了Heka如何使用circularbufferlibrary来跟踪时间序列数据并在仪表盘中生成图形。事实证明，Heka提供的异常检测功能使用相同的循环缓冲区库。
在引擎覆盖下，你提供了一个“异常配置”是如何工作的，这是一个字符串，看起来像一个编程函数调用。异常配置指定应使用哪个异常检测算法。Heka当前支持的算法是标准偏差变化率测试，以及参数（即Gaussian）和非参数Mann-Whitney-Wilcoxon测试。包括在异常配置中的是关于在循环缓冲器数据结构中的哪个列，我们要监视异常行为的信息。随后，解析的异常配置与填充的循环缓冲区数据结构一起被传递到检测模块的检测功能，并且将使用指定的算法来分析循环缓冲器数据。
幸运的是，对于我们的用例，你不必太担心使用异常检测库的所有细节，因为我们使用的SandboxFilters已经处理了硬件。我们需要做的是创建一个异常的配置字符串，并添加到我们的配置部分。例如，以下是我们如何监视HTTP响应状态代码的示例：
```
[http_status]
type="SandboxFilter"
filename="lua_filters/http_status.lua"
ticker_interval=1
preserve_data=true
message_matcher="Type=='nginx.access'"

[http_status.config]
sec_per_row=1
rows=1800
perservation_version=0
anomaly_config='roc("HTTPStatus",2,15,0,1.5,true,false)mww_nonparametric("HTTPStatus",5,15,10,0.8)'
```
一切都与之前的配置相同，除了我们添加了一个anomaly_config设置。这里有很多，所以我们会一次检查一块。首先要注意的是，实际上有两个异常配置被指定。你可以随意添加。它们在这里是为了可读性而分隔的，但这不是绝对必要的，围绕配置参数的括号足以让Heka识别它们。接下来，我们将依次讨论配置。
第一个异常配置本身看起来像这样：
```
roc("HTTPStatus",2,15,0,1.5,true,false)
```
roc部分告诉我们这个配置正在使用变化率算法。每个算法都有自己的一组参数，因此括号内的值是计算变化率所需的值。第一个参数是payload_name，它需要与消息注入到Heka的消息路由器时使用的payload_name值相对应，在此过滤器的情况下为“HTTP状态”。
下一个参数是我们应该观察的循环缓冲区列。我们在这里指定列2，快速查看http_status.lua源代码将显示你是跟踪200个状态代码的列。下一个值指定在分析窗口中应该使用多少个间隔（即循环缓冲行）。我们说15，这意味着我们将检查两个15秒间隔内值之间的变化率。具体来说，我们将比较第2到16行中的数据和第17到31行中的数据（我们总是丢弃当前行，因为它可能还不完整）。
之后，指定在历史分析窗口中使用的间隔数。设置为0表示使用的是整个历史记录，即第32到1800行。其次是标准偏差阈值参数，设置为1.5。因此，放在一起，我们说如果在过去两个15秒间隔内的200个状态响应数量的变化率比在那之前的29分钟的变化率超过1.5个标准偏差，则应该被触发异常报警。
最后两个参数是布尔值。第一个，是否应该在我们停止接收输入数据的情况下触发报警（我们说是）；第二个，是否在我们一段间隔后再次开始接收数据时触发报警（我们说不）。
这是第一个，现在让我们看第二个：
```
mww_nonparametric("HTTPStatus",5,15,10,0.8)
```
mww_nonparametric告诉我们，你可能猜到，这个配置将使用Mann-Whitney-Wilcoxon非参数算法进行这些计算。该算法可用于识别多个数据集之间的相似性（或差异），即使那些数据集具有非高斯分布，例如数据点集是稀疏的情况。
下一个参数告诉我们将要看什么列。在这种情况下，我们使用列5，这是我们存储500范围状态响应或服务器错误。之后是在分析窗口（15）中使用的间隔的数量，随后是要比较的分析窗口的数量（10）。在这种情况下，这意味着我们将检查最后15秒，并将我们在那里找到的先前10个15秒窗口或150秒之前。
最后一个参数称为pstat，它是一个介于0和1之间的浮点值。这告诉我们要寻找什么类型的数据更改。任何超过0.5意味着我们正在寻找增加的趋势，任何低于0.5意味着我们正在寻找一个递减的趋势。我们将其设置为0.8，这显然在增加的趋势范围内。
因此，合起来，这个异常配置意味着我们将观察最后15秒，以查看服务器错误中是否存在异常高峰，与之前的10个间隔相比。如果检测到服务器错误中的相当大的峰值，我们认为它是异常，并且生成报警。
在此示例中，我们仅指定了对HTTP响应状态监视的异常检测，但是stataly_filter也可以使用anomaly_config选项，因此我们可以对statmetric消息中包含的任何statsd数据应用类似的监视。

3.12通知
------
但是，当我们说检测到异常会产生警报时，我们的意思是什么？与Heka中的其他几乎一样，我们真正说的是，消息将被注入消息路由器，然后其他过滤器和输出插件能够监听并用作操作的触发器。
我们在这里不会详细讨论，但是与异常检测模块一起，Heka的Lua环境提供了一个报警模块，用于生成报警消息（通过调节，以确保快速连续的数百个报警实际上不会生成数百个单独的通知）以及注释模块，其使得所述仪表盘基于我们的循环缓冲器数据来对所述图形应用注释。http状态和统计图过滤器都使用这两个过滤器，因此如果你为这些过滤器指定异常配置，那么将输出图形注释，并在检测到异常时生成报警消息。
报警消息不是很有用，如果他们只是流经Heka的消息路由器，但没有什么正在监听他们。因此，让我们设置一个SmtpOutput，当它们通过时发送电子邮件，它将监听报警消息：
```
[alert_smtp_encoder]
type="SandboxEncoder"
filename="lua_encoders/alert.lua"

[SmtpOutput]
message_matcher="Type=='heka.sandbox-output'&&Fields[payload_type]=='alert'"
encoder="alert_smtp_encoder"
send_from="heka@example.com"
send_to=["alert_recipient@example.com"]
auth="Plain"
user="smtpuser"
password="smtpassword"
host="127.0.0.1:25"
```
首先，我们使用由Heka提供的非常简单的编码器实现来指定编码器，其从消息中提取timestamp、hostname、logger和payload，并以文本格式发送这些值。然后我们添加输出本身，监听任何我们的SandboxFilter插件发出的任何报警消息，使用编码器格式化消息正文，以及通过其他配置选项指定的SMTP服务器发送传出邮件消息。
就是这样！我们不会从异常检测报警生成电子邮件通知程序。

3.13一起试试
------
这里看起来像是我们的完整配置，如果我们把它们放在一个单一的文件：
```
[hekad]
maxprocs=2

[StatsdInput]

[StatAccumInput]
ticker_interval=1
emit_in_fields=true

[CarbonOutput]
message_matcher="Type=='heka.statmetric'"
address="mycarbonserver.example.com:2003"
protocol="udp"

[statmetric-influx-encoder]
type="SandboxEncoder"
filename="lua_encoders/statmetric_influx.lua"

[influx]
type="HttpOutput"
message_matcher="Type=='heka.statmetric'"
address="http://myinfluxserver.example.com:8086/db/stats/series"
encoder="statmetric-influx-encoder"
username="influx_username"
password="influx_password"

[stat_graph]
type="SandboxFilter"
filename="lua_filters/stat_graph.lua"
ticker_interval=1
preserve_data=true
message_matcher="Type=='heka.statmetric'"

[stat_graph.config]
num_rows=300
secs_per_row=1
stats="stats.counters.000000.countstats.counters.000001.countstats.counters.000002.count"
stat_labels="counter_0counter_1counter_2"
preservation_version=0

[DashboardOutput]
ticker_interval=1

[nginx_access_logs]
type="LogstreamerInput"
splitter="TokenSplitter"
decoder="nginx_access_decoder"
log_directory="/var/log/nginx"
file_match='access\.log\.?(?P<Index>\d+)?(.gz)?'
priority=["^Index"]

[nginx_access_decoder]
type="SandboxDecoder"
script_type="lua"
filename="lua_decoders/nginx_access.lua"

[nginx_access_decoder.config]
log_format='$remote_addr-$remote_user[$time_local]"$request"$status$body_bytes_sent"$http_referer""$http_user_agent"'
type="nginx.access"

[ESJsonEncoder]
es_index_from_timestamp=true
type_name="%{Type}"

[ElasticSearchOutput]
message_matcher="Type=='nginx.access'"
encoder="ESJsonEncoder"
flush_interval=50

[http_status]
type="SandboxFilter"
filename="lua_filters/http_status.lua"
ticker_interval=1
preserve_data=true
message_matcher="Type=='nginx.access'"

[http_status.config]
sec_per_row=1
rows=1440
perservation_version=0
anomaly_config='roc("HTTPStatus",2,15,0,1.5,true,false)mww_nonparametric("HTTPStatus",5,15,10,0.8)'

[alert_smtp_encoder]
type="SandboxEncoder"
filename="lua_encoders/alert.lua"

[SmtpOutput]
message_matcher="Type=='heka.sandbox-output'&&Fields[payload_type]=='alert'"
encoder="alert_smtp_encoder"
send_from="heka@example.com"
send_to=["alert_recipient@example.com"]
auth="Plain"
user="smtpuser"
password="smtpassword"
host="127.0.0.1:25"
```
这不是太长，但即使如此，可能很好把它分成更小的块。Heka支持使用目录而不是单个文件进行配置；如果指定目录，则以.toml结尾的所有文件将合并在一起并作为单个配置加载，这对于更复杂的部署是优选的。
这个例子绝不意味着是Heka的特征的详尽列表。事实上，我们只是刚刚接触表面。它可以用作开发配置，以满足你自己的需求的起点。如果你有问题或需要帮助，请使用邮件列表，或使用IRC客户端访问irc.mozilla.org的#heka频道。

3.14 hekad配置
------
hekad配置文件指定将加载哪些输入、分割器、解码器、过滤器、编码器和输出。配置文件为TOML格式。TOML看起来非常类似于INI配置格式，但具有稍微更丰富的数据结构和嵌套支持。
如果hekad的配置文件被指定为目录，则所有包含以“.toml”结尾的文件将被加载并合并到一个配置中。不以“.toml”结尾的文件将被忽略。合并将按字母顺序排列，稍后在合并序列中指定的设置将赢得冲突。
配置文件被分成几个部分，每个部分代表一个插件的单个实例。部分名称指定插件的名称，“type”参数指定插件类型;这必须匹配通过pipeline.RegisterPlugin函数注册的类型之一。例如，以下部分描述名为“tcp：5565”的插件，Heka的插件类型“TcpInput”的实例：
```
[tcp:5565]
type="TcpInput"
splitter="HekaFramingSplitter"
decoder="ProtobufDecoder"
address=":5565"
```
如果选择一个插件名称，也恰好是一个插件类型名称，则可以从部分中省略“type”参数，并将指定的名称用作类型。因此，以下部分描述名为“TcpInput”的插件，也是类型“TcpInput”：
```
[TcpInput]
address=":5566"
splitter="HekaFramingSplitter"
decoder="ProtobufDecoder"
```
请注意，拥有多个相同插件类型的实例是可以的，只要它们的配置不会相互干扰。
除了“type”之外的任何值，如上例中的“address”，将被传递到插件进行内部配置（参见插件配置）。
如果插件在启动期间无法加载，hekad将在启动时退出。当hekad运行时，如果插件失败（由于连接丢失，无法写入文件等），那么如果插件支持重新启动，hekad将关闭或重新启动插件。当插件重新启动时，hekad可能会停止接受消息，直到插件恢复操作（这只适用于过滤器/输出插件）。
插件通过实现重新启动界面指定它们支持重新启动（请参阅重启插件）。支持重新启动的插件可以配置重新启动行为。
内部诊断转发器每30秒运行一次，以扫描用于消息的包，以便可以报告并锁定heka插件中可能的错误，以找到无法正确回收包的插件。

3.15全局配置选项
------
你可以选择在配置文件中声明一个[head]部分，为heka守护进程配置一些全局选项。
配置：
* cpuprof(string output_file):
打开hekad CPU分析，输出记录到output_file。
* max_message_loops(uint):
消息可以重新注入系统的最大次数。这用于防止从过滤器到过滤器的无限消息循环；默认值为4。
* max_process_inject(uint):
沙箱过滤ProcessMessage函数消息的最大数量可以在单个调用中注入；默认值为1。
* max_process_duration(uint64):
沙箱过滤器ProcessMessage函数可以在终止前在单个调用中使用的最大纳秒数；默认值为100000。
* max_timer_inject(uint):
沙箱过滤器TimerEvent函数可以在单个调用中注入的消息的最大数量;默认值为10。
* max_pack_idle(string):
持续时间字符串（e.x.“2s”，“2m”，“2h”），指示消息包在被heka泄露之前可以“idle”多长时间。如果太多的包从过滤器或输出的错误中泄漏，则heka将最终停止。此设置指发生时间。
* maxprocs(int):
启用多核使用；默认为1核。更多的核心通常会增加消息吞吐量。最佳性能通常通过将其设置为2x（内核数）来实现。假设每个内核都是超线程的。
* memprof(string output_file):
启用内存分析；输出记录到output_file。
* poolsize(int):
指定可能存在的最大消息池大小。默认是100。
* plugin_chansize(int):
为各种Heka插件指定输入通道的缓冲区大小。默认值为30。
* base_dir(string):
基本工作目录Heka将通过进程和服务器重新启动用于持久存储。hekad进程必须具有对此目录的读取和写入权限。默认为/var/cache/hekad（Windows上为orc：\var\cache\hekad）。
* share_dir(string):
Heka的“共享目录”的根路径，Heka会期望找到它需要消耗的某些资源。hekad进程应该具有对此目录的只读权限。默认为/usr/share/heka（在Windows上为orc：\usr\share\heka）。
0.6版本新特性
* sample_denominator(int):
指定当计算执行某些操作所需的时间（例如ProtobufDecoder对消息进行解码）或路由器将消息与消息匹配器进行比较时，Heka使用采样率的分母。默认为1000，即1000条消息中的一条消息计算持续时间。
0.6版本新特性.
* pid_file(string):
可选指定pidfile的位置，其中将写入正在运行的hekad进程的进程标识。hekad进程必须对父目录具有读写权限（不会自动创建）。成功退出后，pidfile将被删除。如果路径已经存在，将检查包含的pid是否正在运行进程。如果找到一个，当前进程将退出并出现错误。
0.9版67新特性.
* hostname(string):
指定每当它被要求提供本地主机的主机名时使用的主机名。默认为Go的os.Hostname()调用提供的任何内容。
* max_message_size(uint32):
消息的最大值（以字节为单位）可以在处理期间发送。默认为64KiB。
0.10版本新特性.
* log_flags(int):
控制STDOUT和STDERR日志的前缀。公共值为3（日期和时间，默认值）或0（无前缀）。有关详细信息，请参阅‘https://golang.org/pkg/log/#pkg-constantsGo文档’。
* full_buffer_max_retries(int):
当Heka由于缓冲器填充到容量而关闭时，下一次Heka启动时，它将暂时延迟启动，以给缓冲器一个排出机会，以减轻压力。此设置指定最大间隔数（持续时间上限为1秒），在决定问题未解决并继续启动（或关闭）之前，Heka应等待缓冲区大小低于容量的90％。

3.15 hekad.toml示例文件
------
```
[hekad]
maxprocs=4

#Hekadashboardforinternalmetricsandtimeseriesgraphs
[Dashboard]
type="DashboardOutput"
address=":4352"
ticker_interval=15

#Emailalertingforanomalydetection
[Alert]
type="SmtpOutput"
message_matcher="Type=='heka.sandbox-output'&&Fields[payload_type]=='alert'"
send_from="acme-alert@example.com"
send_to=["admin@example.com"]
auth="Plain"
user="smtp-user"
password="smtp-pass"
host="mail.example.com:25"
encoder="AlertEncoder"

#Userfriendlyformattingofalertmessages
[AlertEncoder]
type="SandboxEncoder"
filename="lua_encoders/alert.lua"

#Nginxaccesslogreader
[AcmeWebserver]
type="LogstreamerInput"
log_directory="/var/log/nginx"
file_match='access\.log'
decoder="CombinedNginxDecoder"

#Nginxaccess'combined'logparser
[CombinedNginxDecoder]
type="SandboxDecoder"
filename="lua_decoders/nginx_access.lua"

[CombinedNginxDecoder.config]
user_agent_transform=true
user_agent_conditional=true
type="combined"
log_format='$remote_addr-$remote_user[$time_local]"$request"$status$body_bytes_sent"$http_referer""$http_user_agent"'

#CollectionandvisualizationoftheHTTPstatuscodes
[AcmeHTTPStatus]
type="SandboxFilter"
filename="lua_filters/http_status.lua"
ticker_interval=60
preserve_data=true
message_matcher="Logger=='AcmeWebserver'"

#rateofchangeanomalydetectiononcolumn1(HTTP200)
[AcmeHTTPStatus.config]
anomaly_config='roc("HTTPStatus",1,15,0,1.5,true,false)'
```

3.16使用环境变量
------
如果你希望在配置文件中使用环境变量作为配置值的方法，可以简单地使用％ENV[VARIABLE_NAME]，并且将文本替换为环境变量VARIABLE_NAME的值。
示例：
```
[AMQPInput]
url="amqp://%ENV[USER]:%ENV[PASSWORD]@rabbitmq/"
exchange="testout"
exchangeType="fanout"
```

3.17配置重启行为
------
支持重新启动的插件具有一组选项，用于管理重新启动和退出时出现错误则如何处理。如果首选，插件可以配置为不重新启动，或者它可以重新启动仅100次，或重新启动尝试可以永远进行。一旦超过max_retries，插件会被取消注册，可能触发hekad关闭（取决于插件的can_exit配置）。
添加重新启动配置是通过添加一个配置部分到插件的配置称为retries。少量的抖动将被添加到重新启动尝试之间的延迟。

配置：
* max_jitter(string):
要添加到重新启动之间的延迟最长抖动持续时间。默认情况下，抖动长达500毫秒，将被添加到每个延迟，以确保随着时间的推移更多的均匀重启尝试。
* max_delay(string):
尝试重新启动插件之间的最长延迟。默认为30秒（30秒）。
* delay(string):
重启尝试之间的启动延迟。该值将是指数避退的初始启动延迟，并且上限不大于max_delay。默认为250ms。
* max_retries(int):
放弃和退出插件之前尝试重新启动插件的最大次数。使用0表示无重新尝试，而-1表示继续尝试（注意：如果插件无法重新启动，这将导致hekad永久停止）。默认为-1。
示例：
```
[AMQPOutput]
url="amqp://guest:guest@rabbitmq/"
exchange="testout"
exchange_type="fanout"
message_matcher='Logger=="TestWebserver"'

[AMQPOutput.retries]
max_delay="30s"
delay="250ms"
max_retries=5
```
